---
layout: page
title: research
permalink: /research/
nav: true
nav_order: 2
---

## Publications
- 2025
    - Q. Bertrand, A. Gagneux, M. Massias, R. Emonet, [On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://www.arxiv.org/abs/2506.03719), preprint.
    - A. Gagneux, M. Massias, E. Soubies, R. Gribonval, [Convexity in ReLU Neural Networks: beyond ICNNs?](https://arxiv.org/abs/2501.03017), Journal of Mathematical Imaging and Vision, 2025.
    - C. Pouliquen, M. Massias, T. Vayer, [Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure](https://arxiv.org/abs/2406.09023), ICLR 2025.
    - A. Gagneux, S. Martin, R. Emonet, Q. Bertrand, M. Massias [A Visual Dive into Conditional Flow Matching](https://iclr-blogposts.github.io/2025/blog/conditional-flow-matching/), blogpost track @ICLR2025.

- 2024
    - A. Gagneux, M. Massias, E. Soubies, Proximal Operators of Non Convex SLOPE, submitted.

- 2023
    - J. Larsson, Q. Klopfenstein, M. Massias, J. Wallin, [Coordinate descent for SLOPE](https://arxiv.org/abs/2210.14780), AISTATS 2023.
    - B. Moufad, P.-A. Bannier, Q. Bertrand, Q. Klopfenstein, M. Massias, [skglm: improving scikit-learn for regularized Generalized Linear Models](/assets/pdf/skglm_mloss.pdf), submitted to JMLR OSS.
    - C. Molinari, M. Massias, L. Rosasco, S. Villa, [Iterative regularization for low-complexity regularizers](https://arxiv.org/abs/2202.00420), Numerische Mathematike.
    - C. Pouliquen, P. Gonçalves, M. Massias, T. Vayer, [Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso](https://arxiv.org/abs/2307.02130), GRETSI 2023.

- 2022
    - T. Moreau, M. Massias, A. Gramfort and others, [Benchopt: Reproducible, efficient and collaborative optimization benchmarks](https://arxiv.org/abs/2206.13424), NeuRIPS 2022.
    - Q. Bertrand, Q. Klopfenstein, P.-A. Bannier, G. Gidel, M. Massias, [Beyond L1: Faster and better sparse models with skglm](https://arxiv.org/abs/2204.07826), NeuRIPS 2022.
    - B. Muzellec, K. Sato, M. Massias, T. Suzuki, [Dimension-free convergence rates for gradient Langevin dynamics in RKHS](https://arxiv.org/abs/2003.00306), COLT 2022.


- 2021
    -    Q. Bertrand, Q. Klopfenstein, M. Massias, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
        [Implicit differentiation for fast hyperparameter selection in non-smooth convex learning](https://arxiv.org/abs/2105.01637), JMLR.
        [code](https://github.com/QB3/sparse-ho)
    -     Q. Bertrand, M. Massias, [Anderson acceleration of coordinate descent](https://arxiv.org/abs/2011.10065), AISTATS 2021. [code](https://mathurinm.github.io/andersoncd )
    -     C. Molinari, M. Massias, L. Rosasco, S. Villa,
        [Iterative regularization for convex regularizers](https://arxiv.org/abs/2006.09859), AISTATS 2021. [code](https://LCSL.github.io/iterreg)

- 2020
    -    M. Massias\*, Q. Bertrand\*, A. Gramfort, J. Salmon,
        [Support recovery and sup-norm convergence rates for sparse pivotal estimation](https://arxiv.org/abs/2001.05401), AISTATS 2020.
    -   M. Massias, S. Vaiter, A. Gramfort, J. Salmon,
        [Dual extrapolation for sparse Generalized Linear Models](https://jmlr.org/papers/v21/19-587.html), JMLR. [celer library](https://github.com/mathurinm/celer)

- 2019
    -   P. Ablin, T. Moreau, M. Massias, A. Gramfort,
        [Learning step sizes for unfolded sparse coding](https://arxiv.org/abs/1905.11071), NeurIPS 2019.
    -   Q. Bertrand\*, M. Massias\*, A. Gramfort, J. Salmon,
        [Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso](https://arxiv.org/abs/1902.02509), NeurIPS 2019.
        [code](https://github.com/QBE/clar)

- 2018
    -    M. Massias, A. Gramfort, J. Salmon,
        [Celer: a fast solver for the Lasso with dual extrapolation](http://proceedings.mlr.press/v80/massias18a.html),
        ICML 2018.
        [doc](https://mathurinm.github.io/celer/)
    -    M. Massias, O. Fercoq, A. Gramfort, J. Salmon,
        [Generalized concomitant multi-task Lasso for sparse multimodal regression](http://proceedings.mlr.press/v84/massias18a.html),
        AISTATS 2018.

- 2017
    -    M. Massias, A. Gramfort, J. Salmon,
        [From safe screening rules to working sets for faster Lasso-type solvers](https://arxiv.org/abs/1703.07285),
        OPTML workshop at NIPS 2017.
    -    M. Massias, J. Salmon, A. Gramfort,
        [Gap safe screening rules for faster complex-valued multi-task group Lasso](http://spars2017.lx.it.pt/index_files/papers/SPARS2017_Paper_77.pdf),
        SPARS, Lisbon, 2017.

## PhD. Thesis
- M. Massias, Sparse high dimension linear regression in the presence of heteroscedastic noise: application to magnetoelectric source imaging. Defended on 04/12/2019.
[manuscript](https://tel.archives-ouvertes.fr/tel-02401628)
[slides](/assets/pdf/slides_defense.pdf )


<!-- ## Relevant slides
-    [./assets/pdf/cirm_mathurin.pdf Dual extrapolation], 09032020, Optimization for Machine Learning workshop at CIRM, Luminy.
-    [./assets/pdf/bounds_aistats20_slides.pdf LCSL group meeting], 23012020, Università di Genova.
-    [./assets/pdf/slides_defense.pdf PhD Defense], 04122019, Inria.
-    [./assets/pdf/AIP2019.pdf
        Dual extrapolation for sparse Generalized Linear Models],
        09072019, Applied Inverse Problems conference, Grenoble.
-    [./assets/pdf/uw_mm.pdf
        Celer: fast solver for the Lasso with dual extrapolation],
        11052018, Statistics Seminar at University of Washington.
-    [https://goo.gl/hZRwqi
        Generalized concomitant multi-task Lasso for sparse multimodal regression],
        20102017, Journées GDR MOA MIA (Bordeaux).
        Also presented at PGMO days 2017, Saclay.
-    [https://goo.gl/wACD2h Résolution rapide de problèmes de type Lasso: des règles de safe screening aux working sets] (in French), 05092017, GRETSI.
        Also presented at JDSE 2017 (best presentation award).
-    [https://goo.gl/8m0a8s Faster solvers for sparse multi-task problems], 20032017, IAP (Paris).
        Also presented at CMStats 2017, London. -->
