<!DOCTYPE html>
<!-- _layouts/distill.html --><html>


<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>The Hutchinson trick | Mathurin Massias' webpage</title>
    <meta name="author" content="Mathurin  Massias">
    <meta name="description" content="Mathurin Massias">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="/blog/hutchinson/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


  <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  
</head>

<!-- <d-front-matter>
  <script async type="text/json">{
      "title": "The Hutchinson trick",
      "description": "",
      "published": "November 18, 2024",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
</d-front-matter> -->

<body class="fixed-top-nav">

  <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Mathurin Massias' webpage</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <!-- [#&lt;Jekyll::Page @relative_path=&quot;404.html&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/about.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/blog.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/genmodels.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;index.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;assets/css/main.scss&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/optml.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/otml.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/research.md&quot;&gt;, #&lt;Jekyll::Page @relative_path=&quot;_pages/teaching.md&quot;&gt;, #&lt;Jekyll:Archive @type=category @title=blog @data={&quot;layout&quot;=&gt;&quot;archive-category&quot;}&gt;, #&lt;Jekyll:Archive @type=year @title={:year=&gt;&quot;2024&quot;} @data={&quot;layout&quot;=&gt;&quot;archive-year&quot;}&gt;, #&lt;JekyllRedirectFrom::PageWithoutAFile @relative_path=&quot;redirects.json&quot;&gt;, #&lt;JekyllFeed::PageWithoutAFile @relative_path=&quot;feed.xml&quot;&gt;, #&lt;Jekyll::PageWithoutAFile @relative_path=&quot;sitemap.xml&quot;&gt;, #&lt;Jekyll::PageWithoutAFile @relative_path=&quot;robots.txt&quot;&gt;] -->

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">
              <li class="nav-item ">
                <a class="nav-link" href="/about/">about</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/research/">research</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

  <!-- Content -->
  <div class="post distill">

    <d-title>
      <h1>The Hutchinson trick</h1>
      <p></p>
    </d-title>

    <d-article>
      

      <p>The Hutchinson trick: a cheap way to evaluate the trace of a Jacobian, without computing the Jacobian itself!</p>

<div class="preamble">
$$
\DeclareMathOperator{\DIV}{div}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\jac}{J}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbE}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\diag}{diag}
\newcommand{\norm}[1]{\Vert #1 \Vert}
$$
</div>

<p>Continuous normalizing flows (more in a blog post to come) heavily rely on the computation of the divergence of a network \(f: \bbR^d \to \bbR^d\), aka the trace of its Jacobian:</p>

\[\begin{equation}
\DIV f (x) = \tr \jac_f(x) \triangleq \sum_{i=1}^d \frac{\mathrm{d} f_i}{\mathrm{d} x_i} (x)
\end{equation}\]

<p>Computing this quantity would require evaluating the full Jacobian, which can be done at the prohibitive cost of \(d\) backpropagations (more details below).
It turns out, the trace/divergence can be approximated reasonably well <strong>with a single call to backpropagation</strong>.</p>

<p>For that, let’s forget about Jacobians for a second and take a generic matrix \(A \in \bbR^{d \times d}\).
The Hutchinson trick states that for any random variable \(z \in \bbR^d\) such that \(\bbE[zz^\top] = \Id_d\),</p>

\[\begin{equation}\label{eq:hutchinson}
\tr(A) = \bbE_z [z^\top A z]
\end{equation}\]

<p>It is typically used for \(z\) having iid entries of mean zero and variance, classically standard Gaussian or Rademacher.
The proof is very simple:</p>

\[\begin{align*}
\bbE_z [z^\top A z]
&amp;= \bbE_z [\tr(z^\top A z)]  \quad &amp;\text{(a scalar equals its trace)} \\
&amp;= \bbE_z [\tr (A z z^\top)] \quad  &amp;(\tr(MN) = \tr(NM))  \\
&amp;= \tr \left(\bbE_z [A z z^\top]\right)  \quad &amp;\text{(trace and expectation commute)}  \\
&amp;= \tr \left(A \bbE_z [z z^\top] \right)  \quad &amp;\text{(linearity of expectation)}  \\
&amp;= \tr \left(A \Id_d \right)  \quad &amp;\text{(assumption on $z$ )}  \\
&amp;= \tr \left(A  \right)  \quad &amp;
\end{align*}\]

<p>et voilà!</p>

<h3 id="why-is-it-useful">Why is it useful?</h3>
<p>The numerical benefit is not obvious at first: if one has access to \(A \in \bbR^{d \times d}\), computing its trace costs \(\mathcal{O}(d)\), while the above formula requires \(\mathcal{O}(d^2)\) to compute \(z^\top A z\), not to mention multiple Monte-Carlo samples for the expectation. But there are cases in which one wants to compute the trace of a matrix, without having explicit access to said matrix!</p>

<p>The flagship example is the full Jacobian of a neural network \(f\).
Explicitely computing it is out of reach: with backpropagation one can only compute Jacobian-vector products, aka \(\jac_f(x) v\) for \(v \in \bbR^d\).
To compute the full Jacobian means computing \(\jac_f(x) e_i\) for all canonical vectors \(e_i\), hence calling backpropagation \(d\) times (though it can be done <a href="https://pytorch.org/tutorials/intermediate/jacobians_hessians.html" target="_blank" rel="noopener noreferrer">in a vectorized fashion</a>).
But, to compute the <em>trace</em> of the Jacobian, one can sample a single \(z\), and then approximate the expectation in \eqref{eq:hutchinson} by a (single) Monte-Carlo estimate:</p>

\[\tr J_f(x) \approx z^\top \left(J_f(x) z \right)\]

<p>where \(J_f(x) z\) is computed with a single backpropagation pass. This is what is done in Continuous normalizing flows, in the Ordinary Differential Equation required to evaluate the log likelihood loss. Very clever and elegant!</p>

<h3 id="is-it-a-good-estimator">Is it a good estimator?</h3>

<p>The Hutchinson estimator is a very elegant trick with a short proof, but it has a drawback: its variance is terrible.</p>

<p><strong>Proposition 1</strong>: For standard Gaussian \(z\) the variance of the estimator \(z^\top A z\) is \(2 \norm{A}_F^2\).</p>
<details><summary>Click to expand the proof </summary>
    Wlog assume that \(A\) is symmetric: we can do so because \(A\) has same trace as its symmetric part, and \(z^\top A z = z^\top (A^\top + A) z / 2 \).
    We can use that a Gaussian \(z \) is still Gaussian under orthogonal transform to assume \(A\) is diagonal.
    Then for $A = \Lambda = \diag(\lambda_1, \ldots, \lambda_d)$,
    \begin{align}
        \bbE[(z^\top \Lambda z)^2]
        &amp;= \bbE [(\sum_i \lambda_i z_i^2)^2] \\
        &amp;= \bbE [(\sum_i \lambda_i^2  z_i^4 + \sum_{i \neq j} \lambda_i \lambda_j z_i^2 z_j^2)] \\
        &amp;= 3\sum_i \lambda_i^2  + \sum_{i \neq j} \lambda_i \lambda_j  \\
        &amp;= 2\sum_i \lambda_i^2  + \sum_i \lambda_i^2 + \sum_{i \neq j} \lambda_i \lambda_j  \\
        &amp;= 2\sum_i \lambda_i^2  + \sum_{i, j} \lambda_i \lambda_j  \\
        &amp;= 2 \norm{A}^2_F + (\tr A)^2
    \end{align}
    using that expectation of $z_i^4 = 3 \sigma^2 = 3$, and $z_i^2$ and $z_j^2$ are independent for $i \neq j$ (with expectation equal to 1)

    Since $\bbE[z^\top A z] = \tr A$, the final variance is $2 \norm{A}^2_F$.
</details>
<p><br>
This result is quite disappointing: the variance scales like \(\mathcal{O}(d^2)\), while the trace scales like \(d\). Can we improve this with other random variables? Yes, but not by much!</p>

<p><strong>Proposition 2</strong>: For uniform sign \(z\) (aka Rademacher variables, taking values +1 or -1 with probability 1/2), the variance of the estimator is $2 \sum_{i \neq j} A_{ij}^2$.
It is the law of \(z\) that minimizes the variance of the Hutchinson estimator.</p>

<details><summary> Click to expand the proof </summary>

Again assume that $A$ is symmetric wlog. Since $z_i^2 = 1$,
\begin{align}
    z^\top A z - \bbE[z^\top A z]
    &amp;= \sum_{i \neq j} A_{ij} z_i z_j \\
    &amp;= 2 \sum_{i &lt; j} A_{ij} z_i z_j
\end{align}
The variables $z_i z_j$ $(i &lt; j)$ are independent Rademacher variables, and so $\Var(z_i z_j) = 1$.
The variance to compute is thus
\begin{align}
    \Var \left(2 \sum_{i &lt; j} A_{ij} z_i z_j\right) = 4 \sum_{i &lt; j} A_{ij}^2 \Var(z_i z_j) = 2 \sum_{i \neq j} A_{ij}^2 \enspace.
\end{align}
For the minimal variance result: for any $z$ (not necessarily Rademacher)
\begin{align}
    \bbE[(z^\top A z)^2]
        &amp;= \bbE[\sum_{ijkl}z_i z_j z_k z_l A_{ij} A_{kl}]
        % &amp;= \bbE[\sum_i A_{ii}^2 z_i^4 + \sum_{i, j} z_i^2 z_j^2 (A_{ii}^2 + 2 A_{ij}^2)]
\end{align}
Now the expectation is 0 as soon as one index is not equal to any of the others ($\bbE[z_i] = 0$ + independence).
So we have four cases to consider:
<ul>
<li> $i=j=k=l$, yielding $\sum_i z_i^4A_{ii}^2$ </li>
<li> $i=j$, $k=l$, $i \neq k$, yielding $\sum_{i \neq k} z_i^2 z_k^2 A_{ii} A_{kk}$ </li>
<li> $i=k$, $j=l$, $i \neq j$, yielding $\sum_{i \neq j} z_i^2 z_j^2 A_{ij}^2$</li>
<li> $i=l$, $j=k$, $i \neq j$, yielding $\sum_{i \neq j} z_i^2 z_j^2 A_{ij}^2$ too</li>
</ul>

Hence by renaming $k$ into $j$ in the second case
\begin{align}
    \bbE[(z^\top A z)^2]
        &amp;= \bbE[\sum_i A_{ii}^2 z_i^4 + \sum_{i \neq j} z_i^2 z_j^2 (A_{ii} A_{jj} + 2 A_{ij}^2)] \\
        &amp;= \sum_i A_{ii}^2 \bbE[z_i^4] + \sum_{i \neq j} (A_{ii} A_{jj} + 2 A_{ij}^2)
\end{align}
since $\bbE[z_i^2 z_j^2] = \bbE[z_i^2] \bbE[z_j^2] = 1$ when $i \neq j$.
To minimize $\bbE[(z^\top A z)^2]$ we should thus minimize $\bbE[z_i^4]$, which by Jensen inequality is lower bounded by $\bbE[z_i^2]^2 = 1$.
The value 1 is achieved for $z_i$ being Rademacher.

</details>

<p><br>
In hindsight, this optimality result makes sense: if \(A\) is diagonal, then for any draw of a Rademacher \(z\), \(z^\top A z = \tr A\) !
With a small numerical experiment (entries of \(A \in \bbR^{20 \times 20}\) i.i.d. standard Gaussian) we can check that Rademacher gives a slightly smaller variance.</p>
<div style="text-align: center;">
<img src="/assets/img/hutchinson.svg" style="width:100%; max-width:600px;">
</div>

<p>If we increase the diagonal entries of \(A\) (by 5 here), we see the variance for Gaussian \(z\) increase as predicted by Proposition 1, while the performance for Rademacher \(z\) is unaffected (coherent with Proposition 2)</p>
<div style="text-align: center;">
<img src="/assets/img/hutchinson2.svg" style="width:100%; max-width:600px;">
</div>

<p><br></p>
<h3 id="beyond-trace-computing-the-full-diagonal">Beyond trace: computing the full diagonal</h3>

<p>As my colleague <a href="https://agonon.github.io/" target="_blank" rel="noopener noreferrer">Antoine Gonon</a> pointed to me, the Hutchinson trick can be used to compute all the entries of the diagonal:</p>

\[\bbE_z [(Az) \odot z] = \diag(A)\]

<p>The proof is very similar to the first one. This technique allows evaluating the diagonal of the Hessian of a neural network \(g: \bbR \to \bbR\), using Hessian-vector product (see <a href="https://iclr-blogposts.github.io/2024/blog/bench-hvp/" target="_blank" rel="noopener noreferrer">this great blog post</a>), which has applications in neural network pruning for example. See more in Section 9.3 of <d-cite key="blondel2024elements"></d-cite>.</p>

    </d-article>

    <d-appendix>
      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

  </div>

  <d-bibliography src="/assets/bibliography/biblio.bib"></d-bibliography>


</body>


</html>
