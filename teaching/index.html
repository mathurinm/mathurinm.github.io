<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Teaching | Mathurin Massias’ webpage</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Teaching" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Mathurin Massias" />
<meta property="og:description" content="Mathurin Massias" />
<link rel="canonical" href="/teaching/" />
<meta property="og:url" content="/teaching/" />
<meta property="og:site_name" content="Mathurin Massias’ webpage" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Teaching" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Mathurin Massias","headline":"Teaching","url":"/teaching/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Mathurin Massias&apos; webpage" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Mathurin Massias&#39; webpage</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">Home</a><a class="page-link" href="/research/">Research</a><a class="page-link" href="/teaching/">Teaching</a><a class="page-link" href="/blog/">Blog</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Teaching</h1>
  </header>

  <div class="post-content">
    <h3 id="computation-optimal-transport-for-machine-and-deep-learning">Computation Optimal Transport for Machine and Deep Learning</h3>

<p>Class details are <a href="/otml">here</a>.</p>

<h3 id="optimization-for-large-scale-machine-learning-m2-ens-2022-2023--2023-2024">Optimization for large scale Machine Learning, M2 ENS 2022-2023 &amp; 2023-2024</h3>

<p>The goal of the class is to cover theoretical aspects and practical Python implementations of popular optimization algorithms in machine learning, with a focus on modern topics: huge scale models, automatic differentiation, deep learning, implicit bias, etc.</p>

<p><a href="/assets/2022_ens/class.pdf" target="_blank" rel="noopener noreferrer"><strong>Notes for the class are here</strong></a>.
<a href="./assets/2022_ens/class.pdf"><strong>Notes for the class are here</strong></a>.</p>

<p><strong>Schedule</strong>: From November 21st onwards: Tuesday 08 h 00, Wednesday 13 h 30 (room B1) <strong>**except Wednesday 6th which is moved to Friday 8th</strong>**.</p>

<p><strong>Validation</strong>: some theoretical homeworks, and paper presentation at the end of the class.</p>

<p><strong>Syllabus</strong>:</p>
<ul>
  <li>basics of convex analysis: convex sets, convex functions, strong convexity, smoothness, subdifferential, Fenchel-Legendre transform, infimal convolution, Moreau envelope</li>
  <li>gradient descent and subgradient descent, fixed point iterations, proximal point method (Lab 1)</li>
  <li>acceleration of first order methods: Nesterov and momentum</li>
  <li>algorithms for Deep Learning: stochastic gradient descent, ADAM, Adagrad (Lab 2)</li>
  <li>automatic differentiation</li>
  <li>second order algorithms: Newton and quasi-Newton methods</li>
  <li>duality in ML</li>
  <li>implicit regularization, Bregman geometry, mirror descent</li>
  <li>recent results in non convex optimization</li>
  <li>online learning</li>
  <li>other algorithms: Frank-Wolfe, primal-dual algorithms, variational inclusions, extragradient.</li>
</ul>

<p>[./assets/2022_ens/Lab_logistic_regression.ipynb Lab 1 on logistic regression is here]</p>

<p>[./assets/2022_ens/Lab_3_DL_empty.ipynb Lab 3 on Deep Learning is here]</p>

<p><strong>Resources</strong>:</p>
<ul>
  <li>/Introductory lectures on convex optimization: a basic course/, Y. Nesterov, 2004. A reference book in optimization, updated in 2018: /Lectures on Convex Optimization/.</li>
  <li>/First order methods in optimization/, A. Beck, 2019.</li>
  <li>/Convex optimization: algorithms and complexity/, S. Bubeck, 2015. A short monograph (100 pages) covering basic topics.</li>
</ul>

<h3 id="classes-taught">Classes taught</h3>

<p>Summer schools:</p>
<ul>
  <li>OLISSIPO Winter school: dimensionality reduction with Titouan Vayer (02/2023)</li>
  <li>Convex optimization @Computation and Modelling summer school, WUST 2022 (<a href="./assets/2022_wust/slides_intro.pdf">intro slides</a> and <a href="./assets/2022_wust/exos.pdf">exercises</a>)</li>
</ul>

<p>Since my arrival at ENS de Lyon (Nov. 2021):</p>
<ul>
  <li>36 h on large scale optimization for machine and deep learning (2022-2024), M2 level.</li>
  <li>12 h on optimization and approximation (2023-2024), M1 level.</li>
</ul>

<p>Since 2019, I teach the Python for datascience class (42 h per year) in the X/HEC “Datascience for business” Master, using live coding  inspired by the Software Carpentry workshops. I designed the course from scratch, collaborating  with Joan Massich in 2019, Quentin Bertrand in 2020, Hicham Janati in 2021, Sylvain Combettes in 2022 and Badr Moufad in 2023.</p>

<p>Since 2020 I teach and handle practical sessions and data camps in Ecole Polytechnique’s [https://portail.polytechnique.edu/datascience/en/programs/data-science-starter-program-dssp Executive education].
Topics involved dimension reduction, clustering, scaling computations, visualization and datacamp. I designed 2 full python labs with Erwan Le Pennec on these topics.</p>

<p>From 2017 to 2019, as a grad student, my main teaching activity was the Optimization for datascience class of the <a href="https://www.universite-paris-saclay.fr/formation/master/mathematiques-et-applications/m2-data-sciences">Datascience Master</a>, totalling 2*40 h including 4 h as lecturer.
Amongst others, this involved refactoring of the practical sessions, tutoring of students during office hours, and partaking in the design of the final exam.</p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <!-- <div class="wrapper">

        <h2 class="footer-heading">Mathurin Massias&#39; webpage</h2>

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list">
                    <li class="p-name">Mathurin Massias&#39; webpage</li></ul>
            </div>
            <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/mathurinm"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">mathurinm</span></a></li><li><a href="https://www.twitter.com/mathusmassias"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">mathusmassias</span></a></li></ul>
</div>

            <div class="footer-col footer-col-3">
                <p>Mathurin Massias</p>
            </div>
    </div>

    </div> -->

</footer>
</body>

</html>
