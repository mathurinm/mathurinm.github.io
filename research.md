---
layout: page
title: Research
permalink: /research/
---

## Papers
- 2024
    - A. Gagneux, **M. Massias**, E. Soubies, Automated and Unbiased Coefficient Clustering with Non Convex SLOPE, under review for TMLR.
    - C. Pouliquen, **M. Massias**, T. Vayer, [Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure](https://arxiv.org/abs/2406.09023), submitted.
- 2023
    - J. Larsson, Q. Klopfenstein, **M. Massias**, J. Wallin, [Coordinate descent for SLOPE](https://arxiv.org/abs/2210.14780), AISTATS 2023.
    - B. Moufad, P.-A. Bannier, Q. Bertrand, Q. Klopfenstein, **M. Massias**, [skglm: improving scikit-learn for regularized Generalized Linear Models](/assets/pdf/skglm_mloss.pdf), submitted to JMLR OSS.
    - C. Molinari, **M. Massias**, L. Rosasco, S. Villa, [Iterative regularization for low-complexity regularizers](https://arxiv.org/abs/2202.00420), Numerische Mathematike.
    - C. Pouliquen, P. Gonçalves, **M. Massias**, T. Vayer, [Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso](https://arxiv.org/abs/2307.02130), GRETSI 2023.

- 2022
    - T. Moreau, **M. Massias**, A. Gramfort and others, [Benchopt: Reproducible, efficient and collaborative optimization benchmarks](https://arxiv.org/abs/2206.13424), NeuRIPS 2022.
    - Q. Bertrand, Q. Klopfenstein, P.-A. Bannier, G. Gidel, **M. Massias**, [Beyond L1: Faster and better sparse models with skglm](https://arxiv.org/abs/2204.07826), NeuRIPS 2022.
    - B. Muzellec, K. Sato, **M. Massias**, T. Suzuki, [Dimension-free convergence rates for gradient Langevin dynamics in RKHS](https://arxiv.org/abs/2003.00306), COLT 2022.


- 2021
    -    Q. Bertrand, Q. Klopfenstein, **M. Massias**, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
        [Implicit differentiation for fast hyperparameter selection in non-smooth convex learning](https://arxiv.org/abs/2105.01637), JMLR.
        [code](https://github.com/QB3/sparse-ho)
    -     Q. Bertrand, **M. Massias**, [Anderson acceleration of coordinate descent](https://arxiv.org/abs/2011.10065), AISTATS 2021. [code](https://mathurinm.github.io/andersoncd )
    -     C. Molinari, **M. Massias**, L. Rosasco, S. Villa,
        [Iterative regularization for convex regularizers](https://arxiv.org/abs/2006.09859), AISTATS 2021. [code](https://LCSL.github.io/iterreg)

- 2020
    -    **M. Massias**\**, Q. Bertrand\**, A. Gramfort, J. Salmon,
        [Support recovery and sup-norm convergence rates for sparse pivotal estimation](https://arxiv.org/abs/2001.05401), AISTATS 2020.
    -   **M. Massias**, S. Vaiter, A. Gramfort, J. Salmon,
        [Dual extrapolation for sparse Generalized Linear Models](https://jmlr.org/papers/v21/19-587.html), JMLR. [celer library](https://github.com/mathurinm/celer)

- 2019
    -   P. Ablin, T. Moreau, **M. Massias**, A. Gramfort,
        [Learning step sizes for unfolded sparse coding](https://arxiv.org/abs/1905.11071), NeurIPS 2019.
    -   Q. Bertrand\**, **M. Massias**\**, A. Gramfort, J. Salmon,
        [Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso](https://arxiv.org/abs/1902.02509), NeurIPS 2019.
        [code](https://github.com/QBE/clar)

- 2018
    -    **M. Massias**, A. Gramfort, J. Salmon,
        [Celer: a fast solver for the Lasso with dual extrapolation](http://proceedings.mlr.press/v80/massias18a.html),
        ICML 2018.
        [doc](https://mathurinm.github.io/celer/)
    -    **M. Massias**, O. Fercoq, A. Gramfort, J. Salmon,
        [Generalized concomitant multi-task Lasso for sparse multimodal regression](http://proceedings.mlr.press/v84/massias18a.html),
        AISTATS 2018.

- 2017
    -    **M. Massias**, A. Gramfort, J. Salmon,
        [From safe screening rules to working sets for faster Lasso-type solvers](https://arxiv.org/abs/1703.07285),
        OPTML workshop at NIPS 2017.
    -    **M. Massias**, J. Salmon, A. Gramfort,
        [Gap safe screening rules for faster complex-valued multi-task group Lasso](http://spars2017.lx.it.pt/index_files/papers/SPARS2017_Paper_77.pdf),
        SPARS, Lisbon, 2017.

## PhD. Thesis
- **M. Massias**, Sparse high dimension linear regression in the presence of heteroscedastic noise: application to magnetoelectric source imaging. Defended on 04/12/2019.
[manuscript](https://tel.archives-ouvertes.fr/tel-02401628)
[slides](/assets/pdf/slides_defense.pdf )


<!-- ## Relevant slides
-    [./assets/pdf/cirm_mathurin.pdf Dual extrapolation], 09032020, Optimization for Machine Learning workshop at CIRM, Luminy.
-    [./assets/pdf/bounds_aistats20_slides.pdf LCSL group meeting], 23012020, Università di Genova.
-    [./assets/pdf/slides_defense.pdf PhD Defense], 04122019, Inria.
-    [./assets/pdf/AIP2019.pdf
        Dual extrapolation for sparse Generalized Linear Models],
        09072019, Applied Inverse Problems conference, Grenoble.
-    [./assets/pdf/uw_mm.pdf
        Celer: fast solver for the Lasso with dual extrapolation],
        11052018, Statistics Seminar at University of Washington.
-    [https://goo.gl/hZRwqi
        Generalized concomitant multi-task Lasso for sparse multimodal regression],
        20102017, Journées GDR MOA MIA (Bordeaux).
        Also presented at PGMO days 2017, Saclay.
-    [https://goo.gl/wACD2h Résolution rapide de problèmes de type Lasso: des règles de safe screening aux working sets] (in French), 05092017, GRETSI.
        Also presented at JDSE 2017 (best presentation award).
-    [https://goo.gl/8m0a8s Faster solvers for sparse multi-task problems], 20032017, IAP (Paris).
        Also presented at CMStats 2017, London. -->
