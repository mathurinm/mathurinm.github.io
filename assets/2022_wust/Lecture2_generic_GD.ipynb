{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db390f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import check_grad \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbb3de",
   "metadata": {},
   "source": [
    "## Curse of dimension? How good is the gradient direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = 500, 500\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "b = np.random.randn(n_samples)\n",
    "\n",
    "L = np.linalg.norm(A, ord=2) ** 2\n",
    "def f(x):\n",
    "    return 0.5 * norm(A @ x - b)**2\n",
    "\n",
    "x0 = np.random.randn(n_features)\n",
    "\n",
    "grad_increment = 1/L * A.T @ (A @ x0 - b)\n",
    "f_grad = f(x0 - grad_increment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e5d91",
   "metadata": {},
   "source": [
    "Lets compare the objective values obtained by the gradient steps with the ones obtained by random directions (with same update magnitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d64b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = []\n",
    "for _ in range(1000):\n",
    "    direction = np.random.randn(n_features)\n",
    "    direction *= norm(grad_increment) / norm(direction)\n",
    "    \n",
    "    fs.append(f(x0 - direction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7100c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab10')\n",
    "plt.hist(fs, bins=10)\n",
    "plt.axvline(f(x0), label='$f(x_0)$', c=cmap(1))\n",
    "plt.axvline(f_grad, label='$f(x_0 - 1/L \\\\nabla f(x_0))$', c=cmap(2))\n",
    "plt.legend(loc='upper left', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de142d3a",
   "metadata": {},
   "source": [
    "## GD on exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 10), np.exp(-np.linspace(0, 10)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a93ce5e",
   "metadata": {},
   "source": [
    "- Is the function $x \\mapsto \\exp(-x)$, restricted to $\\mathbb{R}_+$ convex? Strongly convex? Smooth?\n",
    "- And on $\\mathbb{R}$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "n_iter = 100000\n",
    "objs = np.zeros(n_iter)\n",
    "for t in range(n_iter):\n",
    "    x = x + np.exp(-x)  # smoothness constant = sup_x f''(x) = 1  -> stepsize = 1 / 1\n",
    "    objs[t] = np.exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(objs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs[[10, 100, 1000, 10_000, 99_000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2db241",
   "metadata": {},
   "source": [
    "As indicated by the $O(1/k)$ bound, we need roughly 10 times more iterations to get a 10 times better precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c1dbb",
   "metadata": {},
   "source": [
    "## A small puzzle : least squares without strong convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e766fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_samples, n_features = 30, 50\n",
    "b = np.random.randn(n_samples)\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "\n",
    "print(f\"A has {A.shape[0]} rows and {A.shape[1]} columns\")\n",
    "\n",
    "eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "plt.plot(eigvals)\n",
    "plt.xlabel(\"eigenvalue index\", fontsize=14)\n",
    "plt.ylabel(\"eigenvalue of $A^\\\\top A$\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbed6a",
   "metadata": {},
   "source": [
    "Is $x \\mapsto \\frac12 \\Vert Ax - b \\Vert^2$ strongly convex when $A$ has the spectrum above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "L = norm(A, ord=2) ** 2\n",
    "x = np.zeros(A.shape[1])\n",
    "objs = []\n",
    "for it in range(max_iter):\n",
    "    x -= 1. / L * A.T @ (A @ x - b)\n",
    "    objs.append(0.5 * norm(A @ x - b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b511ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf6e48",
   "metadata": {},
   "source": [
    "Why do observe a linear rate even though the objective is not strongly convex? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc214ad",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "For binary labels $b_i \\in \\{-1, 1\\}$ and observations $a_i \\in \\mathbb{R}^d$, the *logistic regression* estimator is defined as the solution of:\n",
    "\n",
    "$$ \\min \\sum_{i=1}^n \\log (1 + \\exp(- b_i a_i^\\top x)) $$\n",
    "\n",
    "Let's investigate gradient descent behavior on this objective function, on **separable** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f90388",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[-1, 0], \n",
    "              [-.5, .5],\n",
    "              [0, -1], \n",
    "              [1, 0]])\n",
    "\n",
    "b = np.array([1, 1, -1, -1])\n",
    "\n",
    "for value, marker in zip([-1, 1], [\"+\", \"o\"]):\n",
    "    points = b == value\n",
    "    plt.scatter(A[points, 0], A[points, 1], marker=marker)\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f0960",
   "metadata": {},
   "source": [
    "We see that there exists many hyperplanes separing exactly the data into positive and negative samples. This means that the iterates of GD will go to infinity, as the infimum of the loss (0) is not attained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6e0ca",
   "metadata": {},
   "source": [
    "The objective value and its gradient can be computed explicitely:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31416aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(x):\n",
    "    return np.log(1. + np.exp(-b * (A @ x))).sum()\n",
    "\n",
    "def grad_logistic_loss(x):\n",
    "    return A.T @ (- b / (1. + np.exp(b * (A @ x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814973",
   "metadata": {},
   "source": [
    "We check that our formula for the gradient is close to the one that `scipy` can compute with finite difference method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grad(logistic_loss, grad_logistic_loss, np.random.randn(A.shape[1])) \n",
    "# norm of difference should be small "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ae8da",
   "metadata": {},
   "source": [
    "The objective is smooth, with Lipschitz constant given by (see exercise sheet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf851de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.linalg.norm(A, ord=2) ** 2 / 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD algorithm:\n",
    "def GD_logreg(stepsize=1/L, n_iter=20):\n",
    "    x = np.ones(A.shape[1])\n",
    "    all_x = np.zeros([n_iter, A.shape[1]])\n",
    "    objs = []\n",
    "    for it in range(n_iter):\n",
    "        all_x[it] = x\n",
    "        objs.append(logistic_loss(x))\n",
    "        x -= stepsize * grad_logistic_loss(x)\n",
    "    return x, all_x, objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatLogSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a736173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iterates(n_iter):\n",
    "    n_iter = int(n_iter)\n",
    "    x, all_x, objs = GD_logreg(1/L, n_iter)\n",
    "    for value, marker in zip([-1, 1], [\"+\", \"o\"]):\n",
    "        points = b == value\n",
    "        plt.scatter(A[points, 0], A[points, 1], marker=marker)\n",
    "\n",
    "\n",
    "    plt.scatter(all_x[:, 0], all_x[:, 1])\n",
    "    plt.axis(\"equal\");\n",
    "\n",
    "interact(plot_iterates, n_iter=FloatLogSlider(base=2, step=1, min=0, max=15));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10c3bd",
   "metadata": {},
   "source": [
    "## GD on strongly convex Least Squares: another puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = 500, 500\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "b = np.random.randn(n_samples)\n",
    "\n",
    "L = np.linalg.norm(A, ord=2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066f41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter = 3000\n",
    "x0 = np.ones(n_features)\n",
    "x = x0.copy()\n",
    "objs = []\n",
    "for it in range(max_iter):\n",
    "    if it % 100 == 0:\n",
    "        print(f'Iter {it}')\n",
    "    x = x - 1. / L * A.T @ (A @ x - b)\n",
    "    objs.append(norm(A @ x - b) ** 2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3548f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(objs)\n",
    "plt.xlabel(\"Iteration $k$\", fontsize=16)\n",
    "plt.ylabel(\"$f(x_k) - f(x^*)$\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a431be7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = np.linalg.eigvalsh(A.T @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dff253",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = eigvals[-1] / eigvals[0]\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.linalg.solve(A, b)\n",
    "it = np.arange(1, max_iter)\n",
    "plt.semilogy(it, objs[1:], label='actual')\n",
    "plt.semilogy(it, 2 * L * norm(x0 - x_star)  ** 2 / it, label='1 / T rate')\n",
    "plt.semilogy(it, L * norm(x0 - x_star) ** 2 * np.exp(- it / kappa), label='linear rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7b5d0",
   "metadata": {},
   "source": [
    "$\\hookrightarrow$ The condition number is huge, and the constants make the 1/k rate better than the linear one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43defd0a",
   "metadata": {},
   "source": [
    "## Nesterov accelerated gradient (momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f51edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = 400, 500\n",
    "np.random.seed(0)\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "b = np.random.randn(n_samples)\n",
    "\n",
    "n_iter = 10000\n",
    "L = np.linalg.norm(A, ord=2) ** 2\n",
    "\n",
    "t = 1\n",
    "x = np.ones(n_features)\n",
    "y = x.copy()\n",
    "x_gd = x.copy()\n",
    "\n",
    "objs_agd = np.zeros(n_iter)\n",
    "objs_gd = np.zeros(n_iter)\n",
    "\n",
    "for it in range(n_iter):\n",
    "    t_old = t\n",
    "    x_old = x\n",
    "    \n",
    "    x = y - 1/L * A.T @ (A @ y - b)\n",
    "    t = (1 + np.sqrt(1 + 4 * t ** 2)) / 2\n",
    "    y = x + (t_old - 1) / t * (x - x_old) \n",
    "    objs_agd[it] = 0.5 * norm(A @ x - b) ** 2\n",
    "    \n",
    "    x_gd -= 1/L * A.T @ (A @ x_gd - b)\n",
    "    objs_gd[it] = norm(A @ x_gd - b) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(objs_agd, label=\"Accelerated GD\")\n",
    "plt.semilogy(objs_gd, label=\"GD\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0706ac0",
   "metadata": {},
   "source": [
    "**Exercise**: increase to the number of iteration to 5000 and check the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b8be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
