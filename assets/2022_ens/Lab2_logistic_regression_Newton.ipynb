{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62288fa5",
   "metadata": {},
   "source": [
    "# Lab 2: Logistic regression, line search and second order methods\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times d}$ and $b \\in \\{-1, 1\\}^n$. \n",
    "The logistic regression estimator for classification is defined as:\n",
    "$$ \\min_{x \\in \\mathbb{R}^d} \\sum_{i=1}^n \\log (1 + e^{- b_i \\ a_i^\\top x}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def make_data(n, d, snr=3, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    A = np.random.randn(n, d)\n",
    "    x_true = np.random.randn(d)\n",
    "    \n",
    "    pred_true = A @ x_true\n",
    "    noise = np.random.randn(n)\n",
    "    pred_true += noise / norm(noise) * norm(pred_true) / snr \n",
    "    return A, np.sign(pred_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d0c8a",
   "metadata": {},
   "source": [
    "0. What does the following function do? To what quantity does each parameter correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = make_data(200, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad18e1b",
   "metadata": {},
   "source": [
    "1. Define a function computing the gradient of the logistic regression objective function at ``x``. Check numerically that it is correct with ``scipy.optimize.check_grad``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "\n",
    "def logistic_gradient(x):\n",
    "    # TODO\n",
    "    # your code to compute the gradient of the logistic regression objective\n",
    "    return\n",
    "\n",
    "def logistic_value(x):\n",
    "    # TODO\n",
    "    # your code to compute the value of the logistic regression objective\n",
    "    return\n",
    "\n",
    "\n",
    "# check for a random vector of coefficient x0\n",
    "x0 = np.random.randn(100)\n",
    "check_grad(logistic_value, logistic_gradient, x0)  # it must be small if your implementation is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559644f",
   "metadata": {},
   "source": [
    "2. What is the smoothness constant of the logistic loss? Is it strongly convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a4856",
   "metadata": {},
   "source": [
    "3. Implement gradient descent on the objective with fixed stepsize. Plot the convergence rate in objective. Does it match your expectations? Compare your solutions with scikit-learn's.\n",
    "Add an option to use line search (using ``scipy.optimize.line_search``). Compare the convergence speed and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853efe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import line_search\n",
    "\n",
    "\n",
    "def solve_with_GD(max_iter, use_line_search=False):\n",
    "    # TODO\n",
    "    # your implementation of gradient descent\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f10594",
   "metadata": {},
   "source": [
    "4. Define a function to compute the Hessian of the logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ef688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_hessian(x):\n",
    "    # TODO\n",
    "    # your code to compute the Hessian of the logistic regression objective\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71186546",
   "metadata": {},
   "source": [
    "5. Implement Newton method without line search on this function. Comment on the possible behavior.\n",
    "Add line search to Newton method using ``scipy.optimize.line_search``. Compare the convergence speed in number of iterations with respect to gradient descent. Compare in terms of time too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_newton(max_iter, use_line_search):\n",
    "    # TODO\n",
    "    # your implementation of Newton method\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3b526",
   "metadata": {},
   "source": [
    "6. Implement the Quasi-Newton method with SR1/Broyden formula seen in class and run it on the logistic loss. Compare with previous algorithms in terms of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea335a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_quasi_Newton(max_iter, use_line_search):\n",
    "    # TODO\n",
    "    # your implementation of Quasi-Newton method\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cf03a",
   "metadata": {},
   "source": [
    "7) Use `scipy.optimize.fmin_lbfgs_b` to solve logistic regression. Compare convergence speed in iterations and time with respect to GD and Newton method. \n",
    "Pass the following callback function to the solver to retrieve time and iterates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import scipy\n",
    "\n",
    "def make_callback():\n",
    "    # closure\n",
    "    times = []\n",
    "    iterates = []\n",
    "    def callback(x):\n",
    "        iterates.append(x.copy())\n",
    "        times.append(time.perf_counter())\n",
    "    return times, iterates, callback\n",
    "\n",
    "times, iterates, callback = make_callback()\n",
    "scipy.optimize.fmin_lbfgs_b(..., callback=callback)  # TODO\n",
    "print(times)\n",
    "print(iterates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5796d1e",
   "metadata": {},
   "source": [
    "8. Compute the Fenchel Rockafellar dual problem of Logistic regression. Does strong duality hold for this problem? \n",
    "What is the relationship between the primal and the dual solutions (use first order optimality condition iv) and v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc94dfe",
   "metadata": {},
   "source": [
    "9. For one of the solvers above (e.g. Newton), plot the objective suboptimality (computing the exact solution with scikit-learn first, for example), together with the duality gap across iterations (using a cleverly chosen dual point). Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86387e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
