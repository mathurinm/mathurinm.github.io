{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942d8e35",
   "metadata": {},
   "source": [
    "# First order methods: subgradient descent, gradient descent, proximal gradient descent \n",
    "\n",
    "Mathurin Massias, Inria     mathurin.massias@inria.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955c10c",
   "metadata": {},
   "source": [
    "## Subgradient descent \n",
    "\n",
    "### Absolute value function\n",
    "\n",
    "Perform subgradient descent for $T$ iterations on the absolute value, with initial point $x_0 = 1$, and fixed stepsize as seen in the class. Describe the behavior.   \n",
    "\n",
    "Increase $T$, comment.\n",
    "\n",
    "Run the algorithm with varying stepsize proportional to $1 / \\sqrt{t}$. Comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfaa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3172cdd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Least absolute deviation (LAD)\n",
    "\n",
    "Generate $A \\in \\mathbb{R}^{10 \\times 15}$ with i.i.d. Gaussian entries. Generate a random Gaussian vector $b \\in \\mathbb{R}^{10}$.\n",
    "\n",
    "Use subgradient descent to minimize $\\Vert Ax - b \\Vert_1$ (the LAD problem). Plot the objective function along iterations. Comment on the converge speed and behavior.\n",
    "\n",
    "On the same plot, compare with gradient descent on Least squares for the same $A$ and $b$. Comment and compare with theoretical rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e1300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e579b7",
   "metadata": {},
   "source": [
    "## Gradient descent on Oridnary Least Squares\n",
    "Generate a 100 by 100 matrix with i.id. normal entries. With propability one, it is invertible and thus $f: x \\mapsto \\frac12 \\Vert Ax - b\\Vert^2$ is $\\mu$-strongly convex.\n",
    "\n",
    "Implement Gradient Descent on $f$ and plot the objective suboptimlaity of the iterates. Do you observe a linear rate? Why?\n",
    "\n",
    "Do the same with $A \\in \\mathbb{R}^{100 \\times 200}$. Is $A^\\top A$ invertible? Is the OLS objective strongly convex?\n",
    "Do the same plot as above. What do you observe? Is it expected? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbcf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7eb4ac",
   "metadata": {},
   "source": [
    "## Proximal gradient descent\n",
    "\n",
    "Implement proximal gradient to solve Non Negative Least Squares: \n",
    "$$\n",
    "\\min_x \\frac12 \\Vert Ax - b\\Vert^2 \\quad \\mathrm{ s.t. } \\quad  x \\geq 0\n",
    "$$\n",
    "\n",
    "Do the same for the Lasso. Pick $\\lambda$ as $\\rho \\Vert X^\\top y \\Vert_\\infty$ with $0 < \\rho < 1$. Check that you obtain the same solution as scikit-learn. \n",
    "$$\n",
    "\\min_x \\frac12 \\Vert Ax - b\\Vert^2  + \\lambda \\Vert x \\Vert_1\n",
    "$$\n",
    "\n",
    "What is the solution when you use $\\lambda = \\Vert X^\\top y \\Vert_\\infty$? Prove it mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e46f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d17c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
