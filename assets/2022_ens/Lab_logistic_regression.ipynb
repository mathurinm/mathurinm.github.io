{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62288fa5",
   "metadata": {},
   "source": [
    "# Lab 1/2: Logistic regression, line search and second order methods\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times d}$ and $b \\in \\{-1, 1\\}^n$. \n",
    "The logistic regression estimator for classification is defined as:\n",
    "$$ \\min_{x \\in \\mathbb{R}^d} \\sum_{i=1}^n \\log (1 + e^{- b_i \\ a_i^\\top x}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def make_data(n, d, corruption=0.1, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    A = np.random.randn(n, d)\n",
    "    x_true = np.random.randn(d)\n",
    "    \n",
    "    b = np.sign(A @ x_true)\n",
    "    switch = np.random.choice(n, int(corruption*n), replace=False)\n",
    "    b[switch] *= - 1\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d0c8a",
   "metadata": {},
   "source": [
    "0. What does the above function do? To what quantity does each parameter correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = make_data(200, 100)  # keep those numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad18e1b",
   "metadata": {},
   "source": [
    "1. Define a function computing the gradient of the logistic regression objective function at ``x``. Check numerically that it is correct with ``scipy.optimize.check_grad``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "\n",
    "def logistic_gradient(x):\n",
    "    # TODO\n",
    "    # your code to compute the gradient of the logistic regression objective\n",
    "    return\n",
    "\n",
    "def logistic_value(x):\n",
    "    # TODO\n",
    "    # your code to compute the value of the logistic regression objective\n",
    "    return\n",
    "\n",
    "\n",
    "# check for a random vector of coefficient x0\n",
    "x0 = np.random.randn(A.shape[1])\n",
    "check_grad(logistic_value, logistic_gradient, x0)  # it must be small if your implementation is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559644f",
   "metadata": {},
   "source": [
    "2. What is the smoothness constant of the logistic loss? Is the logistic loss strongly convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a4856",
   "metadata": {},
   "source": [
    "3. Implement gradient descent on the objective with fixed stepsize $1/L$. Plot the convergence rate in objective. Does it match your expectations? Compare your solutions with scikit-learn's.\n",
    "\n",
    "\n",
    "Add an option to use line search (using ``scipy.optimize.line_search``). Compare the convergence speed and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853efe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import line_search\n",
    "\n",
    "\n",
    "def solve_with_GD(max_iter=500, use_line_search=False):\n",
    "    objectives = []\n",
    "    x = np.zeros(A.shape[1])\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # TODO\n",
    "        # your implementation of gradient descent\n",
    "        objectives[i] = logistic_value(x)\n",
    "        # END TODO\n",
    "        \n",
    "        # break early if no progress made:\n",
    "        if i > 0 and objectives[i-1] - objectives[i] < 1e-8:\n",
    "            print(\"early exit, delta objective:\", objectives[i-1] - objectives[i])\n",
    "            break\n",
    "    return objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=None, fit_intercept=False, tol=1e-8).fit(A, b)\n",
    "\n",
    "f_star = logistic_value(clf.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f10594",
   "metadata": {},
   "source": [
    "4. Define a function to compute the Hessian of the logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ef688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_hessian(x):\n",
    "    # TODO\n",
    "    # your code to compute the Hessian of the logistic regression objective\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71186546",
   "metadata": {},
   "source": [
    "5. Implement Newton method without line search on this function. Comment on the possible behavior.\n",
    "Add line search to Newton method using ``scipy.optimize.line_search``. Compare the convergence speed in number of iterations with respect to gradient descent. Compare in terms of time too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_newton(max_iter, use_line_search):\n",
    "    # TODO\n",
    "    # your implementation of Newton method\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09db60d-2e34-455b-9b06-e6c3620d7b1e",
   "metadata": {},
   "source": [
    "Is the Logistic regression problem strongly convex? Implement an adequate version of Nesterov accelerated gradient on the objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780bceb-f443-4d3b-a554-38d1edaf65df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455e0703-c5e8-4f23-8d6f-04f3b2da521d",
   "metadata": {},
   "source": [
    "Add a regularization term $\\lambda \\Vert x \\Vert^2 / 2$ to the objective, taking $\\lambda = 0.001 L$ for example. What is the conditioning of this problem? Use again an appropriate Nesterov method on this objective; compare the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c8b32-7147-4b1c-a10f-e3093e1fbf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca366f8-e7bb-41e7-a163-6f7317a95c87",
   "metadata": {},
   "source": [
    "Now we consider $\\ell_1$ regularized Logistic regression, ie minimize the sum of the logistic regression loss\n",
    "and $\\lambda \\Vert x \\Vert_1$. Show that for $\\lambda$ greater than $\\Vert A^\\top b \\Vert\\infty/2$, the solution of this problem is 0.\n",
    "\n",
    "Implement proximal gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257f7f0-5ef7-4e35-9922-d53572b7ac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baa678ff-96ae-4968-80c2-a6c29159bdbb",
   "metadata": {},
   "source": [
    "Implement FISTA on the same problem. FISTA consists in replacing the $x$ update step in NAG by:\n",
    "$$_{k+1} = \\mathrm{prox}_{\\gamma g}(y_k - \\gamma \\nabla f(y_k)$$\n",
    "while the $y$ update step is the same as in NAG.\n",
    "Compare the convergence speed with that of ISTA (aka proximal gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f80a51-1771-4773-9e41-037f7cda09f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057628e-8905-45a7-baed-d98496196ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bf4d0-ceb2-4eb4-bd91-e691926457f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc2418-55ef-4a3b-bd63-68e348dd8c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52d9921b-99ab-48c3-8f83-248dbdd24447",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3b526",
   "metadata": {},
   "source": [
    "6. Implement the Quasi-Newton method with SR1/Broyden formula seen in class and run it on the logistic loss. Compare with previous algorithms in terms of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea335a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_quasi_newton(max_iter, use_line_search):\n",
    "    # TODO\n",
    "    # your implementation of Quasi-Newton method\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cf03a",
   "metadata": {},
   "source": [
    "7) Use `scipy.optimize.fmin_lbfgs_b` to solve logistic regression. Compare convergence speed in iterations and time with respect to GD and Newton method. \n",
    "Pass the following callback function to the solver to retrieve time and iterates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import scipy\n",
    "\n",
    "def make_callback():\n",
    "    # closure\n",
    "    times = []\n",
    "    iterates = []\n",
    "    def callback(x):\n",
    "        iterates.append(x.copy())\n",
    "        times.append(time.perf_counter())\n",
    "    return times, iterates, callback\n",
    "\n",
    "times, iterates, callback = make_callback()\n",
    "scipy.optimize.fmin_lbfgs_b(..., callback=callback)  # TODO\n",
    "print(times)\n",
    "print(iterates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5796d1e",
   "metadata": {},
   "source": [
    "8. Compute the Fenchel Rockafellar dual problem of Logistic regression. Does strong duality hold for this problem? \n",
    "What is the relationship between the primal and the dual solutions (use first order optimality condition iv) and v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc94dfe",
   "metadata": {},
   "source": [
    "9. For one of the solvers above (e.g. Newton), plot the objective suboptimality (computing the exact solution with scikit-learn first, for example), together with the duality gap across iterations (using a cleverly chosen dual point). Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86387e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e03df5-e873-4057-ac0a-3b5398b57a3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
